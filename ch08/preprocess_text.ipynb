{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1l7y1tUBE11KS9RqUl3O6CrO0ciPeOIJO",
      "authorship_tag": "ABX9TyNLfpJ/hdQvif8sn9QEH98q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hufsaim/T10402201/blob/master/ch08/preprocess_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzxEAbyU0ea"
      },
      "source": [
        "# Text Preprocessing\n",
        "- https://d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjuLWmspfVEw"
      },
      "source": [
        "import random\n",
        "import re\n",
        "import collections\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL4aloJagcwV"
      },
      "source": [
        "class Vocab:  \n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        counter = count_corpus(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
        "        uniq_tokens += [\n",
        "            token for token, freq in self.token_freqs\n",
        "            if freq >= min_freq and token not in uniq_tokens]\n",
        "        self.idx_to_token, self.token_to_idx = [], dict()\n",
        "        for token in uniq_tokens:\n",
        "            self.idx_to_token.append(token)\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "def count_corpus(tokens):  \n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):  \n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    random.shuffle(initial_indices)\n",
        "    def data(pos):\n",
        "        return corpus[pos:pos + num_steps]\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield torch.tensor(X), torch.tensor(Y)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfwdhTwkBf6"
      },
      "source": [
        "def tokenize(lines, token='word'):  \n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vez1s9tkU-_1"
      },
      "source": [
        "- 임의의 txt 파일을 불러 옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oygLLUpPflEc",
        "outputId": "1ecdef94-e307-47ab-e1f6-5439c32cdb67"
      },
      "source": [
        "path0 = '/content/drive/MyDrive/tmp/timemachine.txt' # replace your own path\n",
        "with open(path0, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "lines\n",
        "[re.sub('[^A-Za-z]+',' ',line).strip().lower() for line in lines]\n",
        "\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# text lines: 3221\n",
            "The Time Machine, by H. G. Wells [1898]\n",
            "\n",
            "twinkled, and his usually pale face was flushed and animated. The\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0lrGTLAVK3N"
      },
      "source": [
        "- line별로 token으로 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reQyEdLbhVin",
        "outputId": "4fa1f60d-d857-455c-fa25-566a747a69e2"
      },
      "source": [
        "tokens = tokenize(lines)\n",
        "for i in range(11):\n",
        "    print(tokens[i])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Time', 'Machine,', 'by', 'H.', 'G.', 'Wells', '[1898]']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['I']\n",
            "[]\n",
            "[]\n",
            "['The', 'Time', 'Traveller', '(for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him)']\n",
            "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us.', 'His', 'grey', 'eyes', 'shone', 'and']\n",
            "['twinkled,', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated.', 'The']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0vs1cjlVXhP"
      },
      "source": [
        "- vocab을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ga7IjPJgdNT",
        "outputId": "a7c5354c-37f9-49a0-8dc7-52fbdc0a5afa"
      },
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('<unk>', 0), ('the', 1), ('I', 2), ('of', 3), ('and', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('my', 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKMhDlL2ggqG",
        "outputId": "2dcc4c3a-54d6-43c0-c9d8-5b1407981670"
      },
      "source": [
        "for i in [0, 10]:\n",
        "    print('words:', tokens[i])\n",
        "    print('indices:', vocab[tokens[i]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words: ['The', 'Time', 'Machine,', 'by', 'H.', 'G.', 'Wells', '[1898]']\n",
            "indices: [16, 27, 330, 32, 2493, 2494, 2495, 2496]\n",
            "words: ['twinkled,', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated.', 'The']\n",
            "indices: [2500, 4, 22, 1034, 462, 168, 7, 1469, 4, 1470, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WpN8r6jf51",
        "outputId": "bb2d64d7-d7c5-4e41-fa45-87ada048feb9"
      },
      "source": [
        "max_tokens = -1\n",
        "corpus = [vocab[token] for line in tokens for token in line]\n",
        "if max_tokens > 0:\n",
        "  corpus = corpus[:max_tokens]\n",
        "  \n",
        "len(corpus), len(vocab)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32306, 6950)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogyzbGAYVdQ_"
      },
      "source": [
        "- 딥러닝모델의 학습에 활용하기 위한 data loader가 제대로 작동하는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzucJR2rloVZ"
      },
      "source": [
        "for X, Y in seq_data_iter_random(corpus[:14], batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rODAr2KCVmPd"
      },
      "source": [
        "- vocab을 통하여, 빈도수 기준 상위 10개의 token을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg1qK9XupAw_",
        "outputId": "8b0e7156-5341-4e8b-e737-1004863a5788"
      },
      "source": [
        "for v in range(0,10):\n",
        "  print(vocab.idx_to_token[v])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\n",
            "the\n",
            "I\n",
            "of\n",
            "and\n",
            "a\n",
            "to\n",
            "was\n",
            "in\n",
            "my\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}